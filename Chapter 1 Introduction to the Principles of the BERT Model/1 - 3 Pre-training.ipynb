{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clarifying the input and output of the model, BERT learns the semantic representation combined with context through two self-supervised learning tasks in the pre-training stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Language Model（MLM）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language model (Language Model, LM) aims to predict subsequent words based on the above, which is a common self-supervised learning task. However, because the language model is one-way, it can only go from left to right or from right to left, and simple two-direction splicing can not really learn two-way and based on the semantic representation of the above and below, so BERT proposed Masked LM . By randomly masking the sequence, replacing the real token with [MASK][MASK], and predicting the real token through the context vector of the token.\n",
    "Take the left-to-right language model as an example, predict the current word $x_t$ according to the previous text ${x_{<t}}$, and the training goal is to maximize the likelihood function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$max_{\\theta} logp_{\\theta}(x) = \\sum_{t=1}^{T} logp_{\\theta}(x_t|x_{<t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLM predicts the masked word based on the input sequence $\\hat{x}$ processed by a random mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$max_{\\theta} logp_{\\theta}(\\bar{x}|\\hat{x}) \\approx \\sum_{t=1}^{T} m_t logp_{\\theta}(x_t|\\hat{x })$\n",
    "Among them, $m_t=1$ means that $x_t$ is masked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specific training data construction method is to randomly extract tokens with a probability of 15% in each data sentence, replace with [MASK] with a probability of 80%, replace with other tokens with a probability of 10%, and keep the probability of 10% unchanged. The reason why it is not completely replaced with [MASK] is to reduce the inconsistency of the input distribution of the pre-training and fine-tuning stages, because the input of the fine-tuning stage does not have [MASK]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Sentence Prediction（NSP）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Masked LM task can learn two-way context relationships. However, many NLP tasks need to understand the relationship between two sentences in addition to understanding a single sentence, such as question answering and reasoning tasks. In order to allow the model to understand the relationship between learning sentence pairs, a second pre-training task NSP is proposed. NSP takes the final output of [CLS] to perform two classifications to determine whether the two sentences currently input are coherent, similar to a language model of sentence granularity, and let the model learn the relationship between input sentence pairs.\n",
    "The specific training data construction method is that for sentence A, sentence B is the next sentence of sentence A with a probability of 50%, and random negative sampling is performed with a probability of 50%, that is, a sentence is randomly selected from the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Author$: Nelson LIN\n",
    "\n",
    "$Email$: nelsonlin0321@outlook.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
